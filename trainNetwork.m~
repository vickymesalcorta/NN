function trainedNetwork = trainNetwork(params)

	% vars declarations
	trainedNetwork = struct();
    
	w = params.w;
	alpha = params.alpha;
	eta = params.eta;
	meanError = 1;
	epocs = 1;
	iter = 1;
	
    % Vector con error de cada iteracion
	trainedNetwork.iterError = [];
  
	% Esto es para usar cuando descarto un w en parametros adaptativos
	lastW = w;
	lastErrorVector = ones(params.training);
	% Empiezo con variacion 0
	
    varW = {params.layers};
	for i = 1:params.layers
	    varW{i}= zeros(size(params.w{i}));
    end
    
    %Para graficar como se va adaptando la función a la esperada
    figure;
    hold on; 
    h_old = plot(0,0);
    %end
     
    while epocs <= params.maxEpocs

	    % 1_ SHUFFLE PATTERNS (input and expected) with the same orderç
        
        % FALTA TESTEAR TODO EL SHUFFLE
  	    shuffleOrder = randperm(params.training);
  	    trainingInput = shufflePatterns(shuffleOrder, params.trainingInput);
  	    trainingExpected = shufflePatterns(shuffleOrder, params.trainingExpected);
        
        %2_ BACKPROP
        errorVector = [];
        for i = 1:params.training
            answer = backPropagation(params, w, i, trainingInput , trainingExpected, eta, alpha, varW);
            w = answer.newW;
            varW = answer.newVarW; 
        end

        test = runTest(params,w);    
        trainedNetwork.iterError(iter) = test.meanError;   

	    if(iter >= 2)
	    	if trainedNetwork.iterError(iter) < trainedNetwork.iterError(iter-1)
	    		% Paso bueno
                if mod(epocs,2) == 0   
                  x = linspace(0,size(toprint,2),size(toprint,2));
                  h = plot(x,trainingExpected,x,toprint);
                  delete(h_old);
                  h_old = h;
                  drawnow;
                end
           
                disp('PASO BUENO');
                disp('error: ');
                disp(trainedNetwork.iterError(iter));
                
  	    		badSteps = 0;
  	    		goodSteps = goodSteps + 1;
                
                %ROLLBACK
                
% 				% Si es un paso bueno, marco este nuevo peso como el ultimo valido
%                 lastLastW = lastW;
%                 lastW = w;
%   				lastErrorVector = errorVector;
                
 				% Desp de adaptStep modificar eta
%   				if goodSteps == params.adaptStep
%   					eta = eta + params.adaptInc;
%   					goodSteps = 0;
%   				end

	    	else
	    		% Paso malo   
 	 %   		alpha = 0;
      			badSteps = badSteps + 1;
     			goodSteps = 0;
%        			eta = (1-params.adaptDec) * eta;
     			iter = iter-1;
%     			
%               % Si es un paso malo, tiro los pesos y vuelvo al anterior
                w = lastW;
                lastW = lastW;
                errorVector = lastErrorVector;
            end
        else
           lastLastW = lastW;
           lastW = w;
        end  
        iter = iter + 1;
        epocs = epocs + 1;
	end

	trainedNetwork.w = w;
	% Error vector es el vector con los errores de cada pattern de la ultima iteracion
	% No confundir con iterError
	trainedNetwork.errorVector = errorVector;
    trainedNetwork.eta = eta;
	trainedNetwork.iter = iter-1;
    trainedNetwork.epocs = epocs-1;
    if badSteps > 0
        w = lastW;
    end
    test = runTest(params,w);    
    % GRAFICO EL OUTPUT DEL TEST VS EL ESPERADO DEL TEST
    x = linspace(0,size(test.result,2),size(test.result,2));
    h = plot(x,test.result,x,params.testExpected);
    delete(h_old);
    drawnow;
    % END
    
end